{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b4588b",
   "metadata": {},
   "source": [
    "# LSTM Pipeline for BTC Price Movement Prediction\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Data Preprocessing**: Feature scaling, correlation analysis, PCA\n",
    "2. **Sequence Construction**: Sliding windows for temporal context\n",
    "3. **LSTM Architecture**: Regularized model with dropout and L2\n",
    "4. **Training Strategy**: Time-series CV, early stopping, LR scheduling\n",
    "5. **Evaluation**: AUC, Precision, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969523f0",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd402d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "GPU Available: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, classification_report, \n",
    "                             confusion_matrix)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb56a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (3377, 24)\n",
      "Date range: 2015-01-31 00:00:00 to 2024-04-29 00:00:00\n",
      "\n",
      "Target distribution:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Target'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTarget distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts(normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     10\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Target'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('BTC_Cleaned_Data.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['Target'].value_counts(normalize=True))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f67bc",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "### 2.1 Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa62351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "# Exclude date and target columns\n",
    "feature_cols = [col for col in df.columns if col not in ['date', 'Target']]\n",
    "X = df[feature_cols].values\n",
    "y = df['Target'].values\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Feature names: {feature_cols}\")\n",
    "print(f\"\\nFeature shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca2382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correlation to identify redundant features\n",
    "correlation_matrix = df[feature_cols].corr()\n",
    "\n",
    "# Find highly correlated features (threshold > 0.9)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i], \n",
    "                correlation_matrix.columns[j], \n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(f\"Highly correlated feature pairs (|corr| > 0.9): {len(high_corr_pairs)}\")\n",
    "for feat1, feat2, corr in high_corr_pairs[:10]:  # Show first 10\n",
    "    print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "    \n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1e625",
   "metadata": {},
   "source": [
    "### 2.2 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55403deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler for LSTM (better for features with outliers)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Feature scaling complete\")\n",
    "print(f\"Scaled data shape: {X_scaled.shape}\")\n",
    "print(f\"Sample statistics after scaling:\")\n",
    "print(f\"  Mean: {X_scaled.mean(axis=0)[:5]}\")  # Show first 5\n",
    "print(f\"  Std: {X_scaled.std(axis=0)[:5]}\")    # Show first 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04bd7d",
   "metadata": {},
   "source": [
    "### 2.3 PCA for Dimensionality Reduction (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce dimensionality while retaining 95% variance\n",
    "USE_PCA = True  # Set to False to skip PCA\n",
    "\n",
    "if USE_PCA:\n",
    "    pca = PCA(n_components=0.95, random_state=42)\n",
    "    X_processed = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    print(f\"PCA applied:\")\n",
    "    print(f\"  Original features: {X_scaled.shape[1]}\")\n",
    "    print(f\"  Reduced features: {X_processed.shape[1]}\")\n",
    "    print(f\"  Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "    print(f\"  Components retained: {pca.n_components_}\")\n",
    "    \n",
    "    # Plot cumulative explained variance\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('PCA - Cumulative Explained Variance')\n",
    "    plt.grid(True)\n",
    "    plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    X_processed = X_scaled\n",
    "    print(\"PCA skipped - using all features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e99061a",
   "metadata": {},
   "source": [
    "## 3. Sequence Construction (Sliding Windows)\n",
    "\n",
    "Create temporal sequences for LSTM input. Each sequence contains multiple days of historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96347a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, sequence_length=30):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM input.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature array (samples, features)\n",
    "        y: Target array (samples,)\n",
    "        sequence_length: Number of time steps to look back\n",
    "        \n",
    "    Returns:\n",
    "        X_seq: Sequences (samples, sequence_length, features)\n",
    "        y_seq: Target values for each sequence\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(sequence_length, len(X)):\n",
    "        X_seq.append(X[i-sequence_length:i])\n",
    "        y_seq.append(y[i])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Create sequences with configurable window size\n",
    "SEQUENCE_LENGTH = 30  # Use 30 days of history (adjustable: 7, 14, 30, 60)\n",
    "\n",
    "X_sequences, y_sequences = create_sequences(X_processed, y, SEQUENCE_LENGTH)\n",
    "\n",
    "print(f\"Sequence creation complete:\")\n",
    "print(f\"  Sequence length: {SEQUENCE_LENGTH} days\")\n",
    "print(f\"  Original samples: {len(X_processed)}\")\n",
    "print(f\"  Sequence samples: {len(X_sequences)}\")\n",
    "print(f\"  Sequence shape: {X_sequences.shape}\")\n",
    "print(f\"  Target shape: {y_sequences.shape}\")\n",
    "print(f\"\\nTarget distribution in sequences:\")\n",
    "print(pd.Series(y_sequences).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f1766",
   "metadata": {},
   "source": [
    "## 4. Time-Series Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663791f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series split: 70% train, 15% validation, 15% test\n",
    "# IMPORTANT: No shuffling to maintain temporal order\n",
    "train_size = int(0.7 * len(X_sequences))\n",
    "val_size = int(0.15 * len(X_sequences))\n",
    "\n",
    "X_train = X_sequences[:train_size]\n",
    "y_train = y_sequences[:train_size]\n",
    "\n",
    "X_val = X_sequences[train_size:train_size+val_size]\n",
    "y_val = y_sequences[train_size:train_size+val_size]\n",
    "\n",
    "X_test = X_sequences[train_size+val_size:]\n",
    "y_test = y_sequences[train_size+val_size:]\n",
    "\n",
    "print(\"Time-series split completed:\")\n",
    "print(f\"  Training samples: {len(X_train)} ({len(X_train)/len(X_sequences)*100:.1f}%)\")\n",
    "print(f\"  Validation samples: {len(X_val)} ({len(X_val)/len(X_sequences)*100:.1f}%)\")\n",
    "print(f\"  Test samples: {len(X_test)} ({len(X_test)/len(X_sequences)*100:.1f}%)\")\n",
    "print(f\"\\nTarget distributions:\")\n",
    "print(f\"  Train - Up: {y_train.mean():.3f}, Down: {1-y_train.mean():.3f}\")\n",
    "print(f\"  Val   - Up: {y_val.mean():.3f}, Down: {1-y_val.mean():.3f}\")\n",
    "print(f\"  Test  - Up: {y_test.mean():.3f}, Down: {1-y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04976509",
   "metadata": {},
   "source": [
    "## 5. Build LSTM Model with Regularization\n",
    "\n",
    "Architecture:\n",
    "- **Input**: Sequences of shape (sequence_length, n_features)\n",
    "- **LSTM Layers**: 1-2 layers with 32-64 units\n",
    "- **Dropout**: 0.3-0.5 for both input and recurrent connections\n",
    "- **L2 Regularization**: On LSTM and Dense layers\n",
    "- **Output**: Binary classification (sigmoid activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eedb542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(sequence_length, n_features, \n",
    "                     lstm_units=[64, 32], \n",
    "                     dropout_rate=0.3,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     l2_reg=0.01,\n",
    "                     learning_rate=0.0005):\n",
    "    \"\"\"\n",
    "    Build LSTM model with regularization.\n",
    "    \n",
    "    Args:\n",
    "        sequence_length: Number of time steps\n",
    "        n_features: Number of features per time step\n",
    "        lstm_units: List of units for each LSTM layer\n",
    "        dropout_rate: Dropout rate for LSTM layers\n",
    "        recurrent_dropout: Recurrent dropout rate\n",
    "        l2_reg: L2 regularization coefficient\n",
    "        learning_rate: Learning rate for Adam optimizer\n",
    "    \"\"\"\n",
    "    model = keras.Sequential(name='LSTM_BTC_Predictor')\n",
    "    \n",
    "    # First LSTM layer\n",
    "    model.add(layers.LSTM(\n",
    "        units=lstm_units[0],\n",
    "        return_sequences=len(lstm_units) > 1,  # Return sequences if more layers\n",
    "        input_shape=(sequence_length, n_features),\n",
    "        dropout=dropout_rate,\n",
    "        recurrent_dropout=recurrent_dropout,\n",
    "        kernel_regularizer=regularizers.l2(l2_reg),\n",
    "        recurrent_regularizer=regularizers.l2(l2_reg),\n",
    "        name='lstm_1'\n",
    "    ))\n",
    "    \n",
    "    # Additional LSTM layers\n",
    "    for i, units in enumerate(lstm_units[1:], start=2):\n",
    "        return_seq = i < len(lstm_units)  # Return sequences if not last layer\n",
    "        model.add(layers.LSTM(\n",
    "            units=units,\n",
    "            return_sequences=return_seq,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            kernel_regularizer=regularizers.l2(l2_reg),\n",
    "            recurrent_regularizer=regularizers.l2(l2_reg),\n",
    "            name=f'lstm_{i}'\n",
    "        ))\n",
    "    \n",
    "    # Dropout layer before output\n",
    "    model.add(layers.Dropout(dropout_rate, name='dropout_final'))\n",
    "    \n",
    "    # Output layer (binary classification)\n",
    "    model.add(layers.Dense(\n",
    "        1, \n",
    "        activation='sigmoid',\n",
    "        kernel_regularizer=regularizers.l2(l2_reg),\n",
    "        name='output'\n",
    "    ))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.AUC(name='auc'),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "n_features = X_train.shape[2]\n",
    "model = build_lstm_model(\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    n_features=n_features,\n",
    "    lstm_units=[64, 32],      # 2-layer LSTM\n",
    "    dropout_rate=0.3,          # 30% dropout\n",
    "    recurrent_dropout=0.2,     # 20% recurrent dropout\n",
    "    l2_reg=0.01,               # L2 regularization\n",
    "    learning_rate=0.0005       # Conservative learning rate\n",
    ")\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "model.summary()\n",
    "print(f\"\\nTotal parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8885d248",
   "metadata": {},
   "source": [
    "## 6. Training Strategy with Callbacks\n",
    "\n",
    "Callbacks:\n",
    "- **EarlyStopping**: Stop training when val_loss stops improving\n",
    "- **ReduceLROnPlateau**: Reduce learning rate when val_loss plateaus\n",
    "- **ModelCheckpoint**: Save best model based on val_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    # Early stopping: stop if val_loss doesn't improve for 10 epochs\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate when val_loss plateaus\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,          # Reduce LR by half\n",
    "        patience=5,          # Wait 5 epochs before reducing\n",
    "        min_lr=1e-7,         # Minimum learning rate\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    ),\n",
    "    \n",
    "    # Save best model based on validation AUC\n",
    "    ModelCheckpoint(\n",
    "        filepath='btc_lstm_best_model.h5',\n",
    "        monitor='val_auc',\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "        mode='max'\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(f\"  - Early Stopping (patience=10)\")\n",
    "print(f\"  - Reduce LR on Plateau (factor=0.5, patience=5)\")\n",
    "print(f\"  - Model Checkpoint (best val_auc)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19bff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Starting model training...\")\n",
    "print(f\"Batch size: 32\")\n",
    "print(f\"Max epochs: 100\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,           # Small batch size for stability\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1856502",
   "metadata": {},
   "source": [
    "### 6.1 Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[0, 1].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "axes[1, 0].plot(history.history['auc'], label='Train AUC', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_auc'], label='Val AUC', linewidth=2)\n",
    "axes[1, 0].set_title('Model AUC', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('AUC')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision & Recall\n",
    "axes[1, 1].plot(history.history['precision'], label='Train Precision', linewidth=2)\n",
    "axes[1, 1].plot(history.history['val_precision'], label='Val Precision', linewidth=2, linestyle='--')\n",
    "axes[1, 1].plot(history.history['recall'], label='Train Recall', linewidth=2)\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linewidth=2, linestyle='--')\n",
    "axes[1, 1].set_title('Precision & Recall', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\nFinal Training Metrics:\")\n",
    "print(f\"  Train Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"  Val Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Train Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"  Val Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"  Train AUC: {history.history['auc'][-1]:.4f}\")\n",
    "print(f\"  Val AUC: {history.history['val_auc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5d0b99",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "Evaluate on test set with comprehensive metrics:\n",
    "- Accuracy\n",
    "- AUC (Area Under ROC Curve)\n",
    "- Precision, Recall, F1-Score\n",
    "- Confusion Matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
